{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import spacy\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import os  # Import os module\n",
    "import shutil # Import shutil for removing directory\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from captum.attr import LayerIntegratedGradients\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# Initialize models\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser', 'lemmatizer'])\n",
    "\n",
    "# Move nlp to GPU (optional but recommended)\n",
    "# nlp.to(device)\n",
    "\n",
    "# Use a BERT model fine-tuned on Amazon reviews\n",
    "model_name = 'nlptown/bert-base-multilingual-uncased-sentiment'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "bert_model = BertModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "class RegularizedBERT(nn.Module):\n",
    "    def __init__(self, num_labels, feature_dim, hyperparams):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model\n",
    "        self.dropout = nn.Dropout(hyperparams[\"Dropout\"])\n",
    "        self.feature_dim = feature_dim\n",
    "        self.bert_projection = nn.Linear(768, 384)\n",
    "        self.feature_projection = nn.Linear(feature_dim, 384) if feature_dim > 0 else None\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(384, 384),\n",
    "            nn.LayerNorm(384),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(hyperparams[\"Dropout\"]),\n",
    "            nn.Linear(384, num_labels)\n",
    "        )\n",
    "        self.all_preds = []\n",
    "        self.all_labels = []\n",
    "        self.all_texts = []\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, features=None):\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = bert_output.last_hidden_state[:, 0, :]\n",
    "        bert_projected = self.bert_projection(pooled_output)\n",
    "        \n",
    "        if features is not None and self.feature_dim > 0:\n",
    "            feature_projected = self.feature_projection(features)\n",
    "            combined_features = bert_projected + feature_projected\n",
    "        else:\n",
    "            combined_features = bert_projected\n",
    "            \n",
    "        output = self.classifier(combined_features)\n",
    "        return output\n",
    "\n",
    "class GPUOptimizedTrainer:\n",
    "    def __init__(self, df, text_column, labels, hyperparams, embedding_dir=\"embeddings\"):\n",
    "        # Validate hyperparameters\n",
    "        required_params = [\"Epochs\", \"Batch Size\", \"Learning Rate\", \"Dropout\", \"Weight Decay\", \n",
    "                         \"Label Smoothing\", \"Early Stopping Patience\", \"Gradient Accumulation Steps\"]\n",
    "        for param in required_params:\n",
    "            if param not in hyperparams:\n",
    "                raise ValueError(f\"Missing required hyperparameter: {param}\")\n",
    "            \n",
    "        if hyperparams[\"Batch Size\"] <= 0:\n",
    "            raise ValueError(\"Batch Size must be positive\")\n",
    "        if not 0 <= hyperparams[\"Dropout\"] <= 1:\n",
    "            raise ValueError(\"Dropout must be between 0 and 1\")\n",
    "            \n",
    "        self.device = device\n",
    "        self.hyperparams = hyperparams\n",
    "        self.batch_size = hyperparams[\"Batch Size\"]\n",
    "        self.embedding_dir = embedding_dir\n",
    "        self.text_column = text_column\n",
    "\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        \n",
    "        # Initialize feature_dim first\n",
    "        sample_features = self.extract_features(df[text_column].head(1).tolist())\n",
    "        self.feature_dim = sample_features.shape[1]\n",
    "        \n",
    "        # Create model with correct feature_dim\n",
    "        self.model = RegularizedBERT(\n",
    "            num_labels=5, \n",
    "            feature_dim=self.feature_dim, \n",
    "            hyperparams=hyperparams\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.scaler = GradScaler()\n",
    "        self.prepare_data(df, labels)\n",
    "        self.setup_training()\n",
    "\n",
    "    def extract_features(self, texts):\n",
    "        print(\"Extracting features...\")\n",
    "        os.makedirs(self.embedding_dir, exist_ok=True) # Ensure embedding directory exists\n",
    "\n",
    "        bert_embedding_file = os.path.join(self.embedding_dir, \"bert_embeddings.npy\")\n",
    "        syntactic_feature_file = os.path.join(self.embedding_dir, \"syntactic_features.npy\")\n",
    "\n",
    "        if os.path.exists(bert_embedding_file) and os.path.exists(syntactic_feature_file):\n",
    "            print(\"Loading embeddings from disk...\")\n",
    "            contextual_features = np.load(bert_embedding_file)\n",
    "            syntactic_features = np.load(syntactic_feature_file)\n",
    "            print(\"Embeddings loaded.\")\n",
    "            return np.hstack([contextual_features, syntactic_features])\n",
    "        else:\n",
    "            print(\"Calculating and saving embeddings...\")\n",
    "\n",
    "            def extract_contextual(texts, batch_size=128):\n",
    "                features = []\n",
    "                for i in range(0, len(texts), batch_size):\n",
    "                    batch = texts[i:i + batch_size]\n",
    "                    # Ensure each text is a string before tokenizing\n",
    "                    batch = [str(text) for text in batch] # <--- ENSURE STRING\n",
    "                    inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "                    with torch.no_grad():\n",
    "                        outputs = bert_model(**inputs)\n",
    "                        embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "                    features.extend(embeddings)\n",
    "                return np.array(features)\n",
    "\n",
    "            def extract_syntactic(texts):\n",
    "                features = []\n",
    "                for text in texts:\n",
    "                    # Ensure each text is a string before passing to nlp\n",
    "                    text = str(text) # <--- ENSURE STRING\n",
    "                    doc = nlp(text)\n",
    "                    pos_tags = [token.pos_ for token in doc]\n",
    "                    features.append([\n",
    "                        len(doc),\n",
    "                        len(set(pos_tags)) / len(pos_tags),\n",
    "                        pos_tags.count('NOUN') / len(pos_tags),\n",
    "                        pos_tags.count('VERB') / len(pos_tags),\n",
    "                    ])\n",
    "                return np.array(features)\n",
    "\n",
    "            contextual_features = extract_contextual(texts)\n",
    "            syntactic_features = extract_syntactic(texts)\n",
    "\n",
    "            np.save(bert_embedding_file, contextual_features) # Save BERT embeddings\n",
    "            np.save(syntactic_feature_file, syntactic_features) # Save syntactic features\n",
    "            print(\"Embeddings calculated and saved.\")\n",
    "            return np.hstack([contextual_features, syntactic_features])\n",
    "\n",
    "    def prepare_data(self, df, labels, val_split=0.2): # Takes dataFrame\n",
    "        print(\"Preparing data...\")\n",
    "\n",
    "        # Extract the data\n",
    "        texts = df[self.text_column].tolist()\n",
    "\n",
    "        # Wrap the text and label processing in a tqdm progress bar\n",
    "        encodings = tokenizer(texts, truncation=True, padding=True, return_tensors='pt')\n",
    "        input_ids = encodings['input_ids']\n",
    "        attention_mask = encodings['attention_mask']\n",
    "        features = self.extract_features(texts) # Features are extracted here\n",
    "        if self.feature_dim is None: # Update feature_dim only once, after extracting features\n",
    "            self.feature_dim = features.shape[1]\n",
    "            self.model.classifier[0] = nn.Linear(768 + self.feature_dim, 384) # Update the first linear layer with correct feature_dim\n",
    "            self.model = self.model.to(self.device) # Move model to device again after changing the layer\n",
    "        scaler = StandardScaler()\n",
    "        features = scaler.fit_transform(features)\n",
    "\n",
    "        split_idx = int(len(input_ids) * (1 - val_split))\n",
    "        indices = np.random.permutation(len(input_ids))\n",
    "\n",
    "        train_idx = indices[:split_idx]\n",
    "        val_idx = indices[split_idx:]\n",
    "\n",
    "        self.train_input_ids = input_ids[train_idx]\n",
    "        self.train_attention_mask = attention_mask[train_idx]\n",
    "        self.train_features = torch.tensor(features[train_idx], dtype=torch.float32)\n",
    "        self.train_labels = torch.tensor(labels[train_idx], dtype=torch.long)\n",
    "        self.train_texts = [texts[i] for i in train_idx] # Store training texts\n",
    "\n",
    "        self.val_input_ids = input_ids[val_idx]\n",
    "        self.val_attention_mask = attention_mask[val_idx]\n",
    "        self.val_features = torch.tensor(features[val_idx], dtype=torch.float32)\n",
    "        self.val_labels = torch.tensor(labels[val_idx], dtype=torch.long)\n",
    "        self.val_texts = [texts[i] for i in val_idx] # Store validation texts\n",
    "\n",
    "\n",
    "        self.create_dataloaders()\n",
    "\n",
    "    def create_dataloaders(self):\n",
    "        train_dataset = TensorDataset(self.train_input_ids, self.train_attention_mask, self.train_features, self.train_labels)\n",
    "        val_dataset = TensorDataset(self.val_input_ids, self.val_attention_mask, self.val_features, self.val_labels)\n",
    "\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            num_workers=4,\n",
    "            prefetch_factor=3,\n",
    "            persistent_workers=True\n",
    "        )\n",
    "\n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.batch_size * 2,  # Validation batch size can be larger\n",
    "            pin_memory=True,\n",
    "            num_workers=4,\n",
    "            persistent_workers=True\n",
    "        )\n",
    "\n",
    "    def setup_training(self):\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.hyperparams[\"Learning Rate\"],\n",
    "            weight_decay=self.hyperparams[\"Weight Decay\"],\n",
    "            betas=(0.9, 0.999)\n",
    "        )\n",
    "\n",
    "        # Total steps for OneCycleLR\n",
    "        total_steps = (len(self.train_loader) // self.hyperparams[\"Gradient Accumulation Steps\"]) * self.hyperparams[\"Epochs\"]\n",
    "\n",
    "        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            self.optimizer,\n",
    "            max_lr=self.hyperparams[\"Learning Rate\"] * 10,  # Usually a good practice to have a higher max_lr\n",
    "            steps_per_epoch=len(self.train_loader) // self.hyperparams[\"Gradient Accumulation Steps\"],\n",
    "            epochs=self.hyperparams[\"Epochs\"],\n",
    "            pct_start=0.1\n",
    "        )\n",
    "\n",
    "        self.early_stopping = EarlyStopping(patience=self.hyperparams[\"Early Stopping Patience\"])\n",
    "\n",
    "    def train(self):\n",
    "        epochs = self.hyperparams[\"Epochs\"]\n",
    "        accumulation_steps = self.hyperparams[\"Gradient Accumulation Steps\"]\n",
    "\n",
    "        # Clear previous predictions\n",
    "        self.model.all_preds = []\n",
    "        self.model.all_labels = []\n",
    "        self.model.all_texts = []\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        train_losses, val_losses = [], []\n",
    "        train_accs, val_accs = [], []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            train_metrics = self.train_epoch(accumulation_steps)\n",
    "            val_metrics = self.validate()\n",
    "\n",
    "            train_losses.append(train_metrics['loss'])\n",
    "            val_losses.append(val_metrics['loss'])\n",
    "            train_accs.append(train_metrics['acc'])\n",
    "            val_accs.append(val_metrics['acc'])\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            print(f\"Train Loss: {train_metrics['loss']:.4f}, Acc: {train_metrics['acc']:.4f}\")\n",
    "            print(f\"Val Loss: {val_metrics['loss']:.4f}, Acc: {val_metrics['acc']:.4f}\")\n",
    "\n",
    "            if val_metrics['loss'] < best_val_loss:\n",
    "                best_val_loss = val_metrics['loss']\n",
    "                torch.save(self.model.state_dict(), 'best_model.pt')\n",
    "\n",
    "            if self.early_stopping(val_metrics['loss']):\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "        self.perform_error_analysis() #Moved to the very end of train\n",
    "        return {\n",
    "            'train_loss': train_losses,\n",
    "            'val_loss': val_losses,\n",
    "            'train_acc': train_accs,\n",
    "            'val_acc': val_accs\n",
    "        }\n",
    "\n",
    "    def train_epoch(self, accumulation_steps):\n",
    "        torch.cuda.empty_cache()\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_idx, (input_ids, attention_mask, features, target) in enumerate(tqdm(self.train_loader, desc=\"Training\", leave=False, dynamic_ncols=True, position=0)): # Explicit position=0\n",
    "            torch.cuda.empty_cache() # Empty cache after every batch\n",
    "            input_ids = input_ids.to(self.device, non_blocking=True)\n",
    "            attention_mask = attention_mask.to(self.device, non_blocking=True)\n",
    "            features = features.to(self.device, non_blocking=True)\n",
    "            target = target.to(self.device, non_blocking=True)\n",
    "\n",
    "            with autocast():\n",
    "                output = self.model(input_ids=input_ids, attention_mask=attention_mask, features=features)\n",
    "                loss = F.cross_entropy(output, target, label_smoothing=self.hyperparams[\"Label Smoothing\"])\n",
    "                loss = loss / accumulation_steps  # Normalize loss for accumulation\n",
    "\n",
    "            self.scaler.scale(loss).backward()\n",
    "\n",
    "            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(self.train_loader):\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "                self.optimizer.zero_grad(set_to_none=True)\n",
    "                self.scheduler.step()\n",
    "\n",
    "            total_loss += loss.item() * accumulation_steps  # Scale back for correct averaging\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "        return {'loss': total_loss / len(self.train_loader), 'acc': correct / total}\n",
    "\n",
    "    def validate(self):\n",
    "        torch.cuda.empty_cache()\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (input_ids, attention_mask, features, target) in enumerate(tqdm(self.val_loader, desc=\"Validating\", leave=False, dynamic_ncols=True, position=0)): # Explicit position=0\n",
    "                torch.cuda.empty_cache() # Empty cache after every batch\n",
    "                input_ids = input_ids.to(self.device, non_blocking=True)\n",
    "                attention_mask = attention_mask.to(self.device, non_blocking=True)\n",
    "                features = features.to(self.device, non_blocking=True)\n",
    "                target = target.to(self.device, non_blocking=True)\n",
    "\n",
    "                with autocast():\n",
    "                    output = self.model(input_ids=input_ids, attention_mask=attention_mask, features=features)\n",
    "                    loss = F.cross_entropy(output, target)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                pred = output.argmax(dim=1)\n",
    "                correct += pred.eq(target).sum().item()\n",
    "                total += target.size(0)\n",
    "\n",
    "                # Get the text for the current batch\n",
    "                start_index = batch_idx * self.batch_size * 2 #validation batch_size 2x the training\n",
    "                end_index = start_index + target.size(0)\n",
    "                batch_texts = self.val_texts[start_index:end_index]\n",
    "\n",
    "\n",
    "                # Store predictions and labels for error analysis\n",
    "                self.model.all_preds.extend(pred.cpu().numpy())\n",
    "                self.model.all_labels.extend(target.cpu().numpy())\n",
    "                self.model.all_texts.extend(batch_texts) # Store the corresponding texts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return {'loss': total_loss / len(self.val_loader), 'acc': correct / total}\n",
    "    def perform_error_analysis(self):\n",
    "        # Generate classification report\n",
    "        report = classification_report(self.model.all_labels, self.model.all_preds, digits=4) #4 for a more detailed overview\n",
    "        print(\"Classification Report:\\n\", report)\n",
    "\n",
    "        # Generate confusion matrix\n",
    "        cm = confusion_matrix(self.model.all_labels, self.model.all_preds)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.xlabel('Predicted Labels')\n",
    "        plt.ylabel('True Labels')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "\n",
    "        # Find misclassified texts\n",
    "        misclassified_indices = np.where(np.array(self.model.all_preds) != np.array(self.model.all_labels))[0]\n",
    "\n",
    "        print(\"\\nMisclassified Text Examples:\")\n",
    "        for idx in misclassified_indices[:5]:  # Display up to 5 examples\n",
    "            print(f\"True Label: {self.model.all_labels[idx]}, Predicted Label: {self.model.all_preds[idx]}\")\n",
    "            print(f\"Text: {self.model.all_texts[idx]}\\n\")\n",
    "\n",
    "        # Explain the predictions using Captum\n",
    "        #print(\"\\nAttribution Analysis for Misclassified Examples:\")\n",
    "        #for idx in misclassified_indices[:5]:  # Display up to 5 examples\n",
    "        #    self.explain_instance(self.model.all_texts[idx], self.model.all_labels[idx], self.model) # Changed all_preds[idx] to all_labels[idx]\n",
    "\n",
    "    def explain_instance(self, text, true_label, model):\n",
    "       #Explain instance no longer prints so is left empty\n",
    "       pass\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        return False\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your DataFrame\n",
    "    df = pd.read_csv(\"/content/modified_df_11.csv\")\n",
    "\n",
    "    # Ensure 'text' column is string\n",
    "    df['text'] = df['text'].astype(str)\n",
    "\n",
    "    #Set name of text column\n",
    "    text_column = 'text'\n",
    "\n",
    "    # Extract labels\n",
    "    labels = np.array(df[\"rating\"].tolist()) - 1  # Adjust as needed\n",
    "\n",
    "    # Define hyperparameters\n",
    "    hyperparams = {\n",
    "        \"Epochs\": 1, #Reduced for faster example\n",
    "        \"Batch Size\": 70,\n",
    "        \"Learning Rate\": 1e-5,\n",
    "        \"Dropout\": 0.3,\n",
    "        \"Weight Decay\": 0.1,\n",
    "        \"Label Smoothing\": 0.1,\n",
    "        \"Early Stopping Patience\": 3,\n",
    "        \"Gradient Accumulation Steps\": 4,\n",
    "        \"Optimizer\": \"AdamW\",\n",
    "        \"Scheduler\": \"OneCycleLR\",\n",
    "        \"Feature Dimension\": 0,\n",
    "        \"Model\": \"BERT with Multiple Embeddings\"\n",
    "    }\n",
    "\n",
    "    embedding_dir = \"my_embeddings\"\n",
    "\n",
    "    # Clear embedding directory\n",
    "    #if os.path.exists(embedding_dir):\n",
    "    #    shutil.rmtree(embedding_dir)\n",
    "\n",
    "    # Instantiate Trainer  Pass the DataFrame, text column name, and TextProcessor\n",
    "    trainer = GPUOptimizedTrainer(df, text_column, labels, hyperparams, embedding_dir=embedding_dir)\n",
    "    metrics = trainer.train()\n",
    "\n",
    "    # Print Dataframe\n",
    "    print(df)\n",
    "\n",
    "    # Plot results\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(metrics['train_loss'], label='Train Loss')\n",
    "    plt.plot(metrics['val_loss'], label='Val Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(metrics['train_acc'], label='Train Acc')\n",
    "    plt.plot(metrics['val_acc'], label='Val Acc')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
