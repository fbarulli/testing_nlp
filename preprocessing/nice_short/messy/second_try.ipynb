{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "df = pd.read_csv('/Users/notagain/data_preprocessing/so_many_rev.csv')\n",
    "#original = pd.read_csv('/Users/notagain/data_preprocessing/text_cleaning/original.csv')\n",
    "#from second_try import ContractionExpander\n",
    "#expander = ContractionExpander()\n",
    "#df_expanded = expander.expand_dataframe(df, 'text')\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def count_words(df: pd.DataFrame, text_column: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Count occurrences of each word across all texts in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the texts\n",
    "        text_column: Name of column containing text\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with word counts sorted by frequency\n",
    "    \"\"\"\n",
    "    word_counts = Counter()\n",
    "    \n",
    "    # Process each text and count words\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        text = str(row[text_column]).lower()  # Convert to string and lowercase\n",
    "        words = text.split()\n",
    "        word_counts.update(words)\n",
    "    \n",
    "    # Convert to DataFrame and sort\n",
    "    word_count_df = pd.DataFrame({\n",
    "        'word': list(word_counts.keys()),\n",
    "        'count': list(word_counts.values())\n",
    "    })\n",
    "    \n",
    "    return word_count_df.sort_values('count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "#word_counts = count_words(df, 'text')\n",
    "#word_counts.to_csv('/Users/notagain/data_preprocessing/text_cleaning/original_word_counts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contraction Analysis:\n",
      "------------------------------------------------------------\n",
      "Contraction     Original   Remaining  Success Rate\n",
      "------------------------------------------------------------\n",
      "its             1403       0          100.0%\n",
      "it's            5639       0          100.0%\n",
      "that's          1270       0          100.0%\n",
      "there's         515        0          100.0%\n",
      "he's            275        0          100.0%\n",
      "she's           103        0          100.0%\n",
      "what's          227        0          100.0%\n",
      "i'm             5366       0          100.0%\n",
      "i've            3453       0          100.0%\n",
      "i'll            916        0          100.0%\n",
      "i'd             859        0          100.0%\n",
      "don't           5573       0          100.0%\n",
      "doesn't         1548       0          100.0%\n",
      "didn't          5578       0          100.0%\n",
      "won't           1184       0          100.0%\n",
      "can't           2328       0          100.0%\n",
      "couldn't        1405       0          100.0%\n",
      "shouldn't       210        0          100.0%\n",
      "wouldn't        1005       0          100.0%\n",
      "they're         546        0          100.0%\n",
      "they've         174        0          100.0%\n",
      "they'll         117        0          100.0%\n",
      "we're           305        0          100.0%\n",
      "we've           281        0          100.0%\n",
      "we'll           145        0          100.0%\n",
      "you're          756        0          100.0%\n",
      "you've          140        0          100.0%\n",
      "you'll          192        0          100.0%\n",
      "let's           87         0          100.0%\n",
      "\n",
      "Word Counts:\n",
      "------------------------------------------------------------\n",
      "Original text: 3604780\n",
      "Expanded text: 3643949\n",
      "Difference: 39169\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_all_contractions(df, text_column='text'):\n",
    "    \"\"\"\n",
    "    Analyze all contractions before and after expansion in a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        text_column: Name of the text column to analyze\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (Results dictionary, expanded DataFrame)\n",
    "    \"\"\"\n",
    "    # Make a copy of original DataFrame\n",
    "    df_expanded = df.copy()\n",
    "    \n",
    "    # Define contractions with their patterns and expansions\n",
    "    # Added the standard ASCII apostrophe (U+0027) explicitly\n",
    "    contractions = {\n",
    "        \"its\": (r\"\\b(?<!th)its\\b(?!\\s+own\\b)\", \"it is\"),  # added pattern for 'its', excluding 'this' and 'its own'\n",
    "        \"it's\": (r\"it[''\\u2019]s\", \"it is\"),\n",
    "        \"that's\": (r\"that[''\\u2019]s\", \"that is\"),\n",
    "        \"there's\": (r\"there[''\\u2019]s\", \"there is\"),\n",
    "        \"he's\": (r\"he[''\\u2019]s\", \"he is\"),\n",
    "        \"she's\": (r\"she[''\\u2019]s\", \"she is\"),\n",
    "        \"what's\": (r\"what[''\\u2019]s\", \"what is\"),\n",
    "        \"i'm\": (r\"i[''\\u2019]m\", \"i am\"),\n",
    "        \"i've\": (r\"i[''\\u2019]ve\", \"i have\"),\n",
    "        \"i'll\": (r\"i[''\\u2019]ll\", \"i will\"),\n",
    "        \"i'd\": (r\"i[''\\u2019]d\", \"i would\"),\n",
    "        \"don't\": (r\"don[''\\u2019]t\", \"do not\"),\n",
    "        \"doesn't\": (r\"doesn[''\\u2019]t\", \"does not\"),\n",
    "        \"didn't\": (r\"didn[''\\u2019]t\", \"did not\"),\n",
    "        \"won't\": (r\"won[''\\u2019]t\", \"will not\"),\n",
    "        \"can't\": (r\"can[''\\u2019]t\", \"cannot\"),\n",
    "        \"couldn't\": (r\"couldn[''\\u2019]t\", \"could not\"),\n",
    "        \"shouldn't\": (r\"shouldn[''\\u2019]t\", \"should not\"),\n",
    "        \"wouldn't\": (r\"wouldn[''\\u2019]t\", \"would not\"),\n",
    "        \"they're\": (r\"they[''\\u2019]re\", \"they are\"),\n",
    "        \"they've\": (r\"they[''\\u2019]ve\", \"they have\"),\n",
    "        \"they'll\": (r\"they[''\\u2019]ll\", \"they will\"),\n",
    "        \"we're\": (r\"we[''\\u2019]re\", \"we are\"),\n",
    "        \"we've\": (r\"we[''\\u2019]ve\", \"we have\"),\n",
    "        \"we'll\": (r\"we[''\\u2019]ll\", \"we will\"),\n",
    "        \"you're\": (r\"you[''\\u2019]re\", \"you are\"),\n",
    "        \"you've\": (r\"you[''\\u2019]ve\", \"you have\"),\n",
    "        \"you'll\": (r\"you[''\\u2019]ll\", \"you will\"),\n",
    "        \"let's\": (r\"let[''\\u2019]s\", \"let us\")\n",
    "    }\n",
    "    \n",
    "    # Rest of the function remains the same\n",
    "    results = {\n",
    "        'contractions': {},\n",
    "        'word_counts': {\n",
    "            'original': 0,\n",
    "            'expanded': 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Count original contractions and words\n",
    "    for text in df[text_column]:\n",
    "        text = str(text)\n",
    "        results['word_counts']['original'] += len(text.split())\n",
    "        \n",
    "        # Count each contraction type\n",
    "        for contraction, (pattern, _) in contractions.items():\n",
    "            count = len(re.findall(pattern, text, re.IGNORECASE))\n",
    "            if contraction not in results['contractions']:\n",
    "                results['contractions'][contraction] = {'original': 0, 'expanded': 0}\n",
    "            results['contractions'][contraction]['original'] += count\n",
    "    \n",
    "    # Expand contractions\n",
    "    def expand_text(text):\n",
    "        text = str(text)\n",
    "        for _, (pattern, expansion) in contractions.items():\n",
    "            text = re.sub(pattern, expansion, text, flags=re.IGNORECASE)\n",
    "        return text\n",
    "    \n",
    "    # Apply expansion\n",
    "    df_expanded[text_column] = df_expanded[text_column].apply(expand_text)\n",
    "    \n",
    "    # Count remaining contractions and expanded words\n",
    "    for text in df_expanded[text_column]:\n",
    "        text = str(text)\n",
    "        results['word_counts']['expanded'] += len(text.split())\n",
    "        \n",
    "        # Count remaining contractions\n",
    "        for contraction, (pattern, _) in contractions.items():\n",
    "            count = len(re.findall(pattern, text, re.IGNORECASE))\n",
    "            results['contractions'][contraction]['expanded'] += count\n",
    "    \n",
    "    # Calculate success rate for each contraction\n",
    "    for contraction in results['contractions']:\n",
    "        original = results['contractions'][contraction]['original']\n",
    "        expanded = results['contractions'][contraction]['expanded']\n",
    "        results['contractions'][contraction]['success_rate'] = \\\n",
    "            100 * (original - expanded) / original if original > 0 else 100\n",
    "    \n",
    "    return results, df_expanded\n",
    "\n",
    "def print_analysis(results):\n",
    "    \"\"\"Print the analysis results in a readable format\"\"\"\n",
    "    print(\"\\nContraction Analysis:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Contraction':<15} {'Original':<10} {'Remaining':<10} {'Success Rate':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for contraction, counts in results['contractions'].items():\n",
    "        if counts['original'] > 0:  # Only show contractions that were present\n",
    "            print(f\"{contraction:<15} {counts['original']:<10} {counts['expanded']:<10} {counts['success_rate']:.1f}%\")\n",
    "    \n",
    "    print(\"\\nWord Counts:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Original text: {results['word_counts']['original']}\")\n",
    "    print(f\"Expanded text: {results['word_counts']['expanded']}\")\n",
    "    print(f\"Difference: {results['word_counts']['expanded'] - results['word_counts']['original']}\")\n",
    "\n",
    "\n",
    "results, expanded_df = analyze_all_contractions(df)\n",
    "print_analysis(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60875/60875 [00:05<00:00, 11757.52it/s]\n"
     ]
    }
   ],
   "source": [
    "exp_count = count_words(expanded_df, 'text')\n",
    "exp_count.to_csv('/Users/notagain/data_preprocessing/text_cleaning/exp_count.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis saved to: number_word_analysis_20250205_085313.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60875/60875 [00:05<00:00, 11684.83it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def analyze_number_replacement(df, text_column='text'):\n",
    "    \"\"\"\n",
    "    Replace numbers with 'number' in text and analyze word counts before and after.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input DataFrame\n",
    "        text_column (str): Name of text column to analyze\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (Modified DataFrame, Word count analysis DataFrame, Analysis results dict)\n",
    "    \"\"\"\n",
    "    # Copy the input DataFrame\n",
    "    df_modified = df.copy()\n",
    "    \n",
    "    # Initialize word count storage\n",
    "    original_word_counts = []\n",
    "    modified_word_counts = []\n",
    "    total_numbers_replaced = 0\n",
    "    \n",
    "    # Regular expression for numbers (positive/negative, integers/decimals)\n",
    "    number_pattern = r'-?\\d*\\.?\\d+'\n",
    "    \n",
    "    # Process each text entry\n",
    "    for text in df[text_column]:\n",
    "        # Convert to string and get original word count\n",
    "        original_text = str(text)\n",
    "        original_word_counts.append(len(original_text.split()))\n",
    "        \n",
    "        # Count numbers in original text\n",
    "        numbers_found = len(re.findall(number_pattern, original_text))\n",
    "        total_numbers_replaced += numbers_found\n",
    "        \n",
    "        # Replace numbers and get new word count\n",
    "        modified_text = re.sub(number_pattern, 'number', original_text)\n",
    "        modified_word_counts.append(len(modified_text.split()))\n",
    "    \n",
    "    # Create modified DataFrame\n",
    "    df_modified[text_column] = df[text_column].apply(\n",
    "        lambda x: re.sub(number_pattern, 'number', str(x))\n",
    "    )\n",
    "    \n",
    "    # Create analysis DataFrame\n",
    "    analysis_df = pd.DataFrame({\n",
    "        'original_text': df[text_column],\n",
    "        'modified_text': df_modified[text_column],\n",
    "        'original_word_count': original_word_counts,\n",
    "        'modified_word_count': modified_word_counts,\n",
    "        'word_count_difference': [m - o for m, o in zip(modified_word_counts, original_word_counts)]\n",
    "    })\n",
    "    \n",
    "    # Create results dictionary for printing\n",
    "    results = {\n",
    "        'word_counts': {\n",
    "            'original': sum(original_word_counts),\n",
    "            'modified': sum(modified_word_counts)\n",
    "        },\n",
    "        'number_stats': {\n",
    "            'total_numbers': total_numbers_replaced,\n",
    "            'total_rows': len(df),\n",
    "            'avg_numbers_per_row': total_numbers_replaced / len(df) if len(df) > 0 else 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save analysis to CSV\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f'number_word_analysis_{timestamp}.csv'\n",
    "    analysis_df.to_csv(filename, index=False)\n",
    "    print(f\"Analysis saved to: {filename}\")\n",
    "    \n",
    "    return df_modified, analysis_df, results\n",
    "\n",
    "def print_analysis(results):\n",
    "    \"\"\"Print the number replacement analysis results in a readable format\"\"\"\n",
    "    print(\"\\nNumber Replacement Analysis:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Metric':<25} {'Value':<15}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Number statistics\n",
    "    print(f\"{'Total numbers replaced':<25} {results['number_stats']['total_numbers']:<15}\")\n",
    "    print(f\"{'Total rows processed':<25} {results['number_stats']['total_rows']:<15}\")\n",
    "    print(f\"{'Avg numbers per row':<25} {results['number_stats']['avg_numbers_per_row']:.2f}\")\n",
    "    \n",
    "    print(\"\\nWord Counts:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Original text total: {results['word_counts']['original']}\")\n",
    "    print(f\"Modified text total: {results['word_counts']['modified']}\")\n",
    "    print(f\"Difference: {results['word_counts']['modified'] - results['word_counts']['original']}\")\n",
    "\n",
    "\n",
    "modified_df_1, analysis, results = analyze_number_replacement(expanded_df)\n",
    "\n",
    "num_count = count_words(modified_df_1, 'text')\n",
    "num_count.to_csv('/Users/notagain/data_preprocessing/text_cleaning/number_count.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text batches: 100%|██████████| 33/33 [01:45<00:00,  3.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text Replacement Analysis:\n",
      "------------------------------------------------------------\n",
      "Metric                    Value          \n",
      "------------------------------------------------------------\n",
      "Total replacements        119249         \n",
      "Total rows processed      60875          \n",
      "Avg replacements/row      1.96\n",
      "\n",
      "Word Counts:\n",
      "------------------------------------------------------------\n",
      "Original text total: 3643949\n",
      "Modified text total: 3643892\n",
      "Difference: -57\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from functools import lru_cache\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Dict, Tuple\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "class TextPatternReplacer:\n",
    "    \"\"\"A class to efficiently handle text pattern replacement and analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: str = '.cache'):\n",
    "        \"\"\"Initialize the replacer with compiled patterns and cache\"\"\"\n",
    "        # Set up caching directory\n",
    "        self.cache_dir = cache_dir\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        \n",
    "        # Compile patterns once during initialization\n",
    "        self.patterns = self._compile_patterns()\n",
    "        \n",
    "        # Initialize result cache\n",
    "        self.result_cache = {}\n",
    "        \n",
    "        # Determine optimal chunk size based on CPU count\n",
    "        self.cpu_count = multiprocessing.cpu_count()\n",
    "        \n",
    "    def _compile_patterns(self) -> Dict[str, re.Pattern]:\n",
    "        \"\"\"Compile regex patterns for better performance\"\"\"\n",
    "        pattern_strings = {\n",
    "            'numbers': r'-?\\d*\\.?\\d+',\n",
    "            'ordinals': r'\\d+(?:st|nd|rd|th)',\n",
    "            'written_numbers': r'zero|one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|twenty|thirty|forty|fifty|sixty|seventy|eighty|ninety|hundred|thousand|million|billion|trillion',\n",
    "            'fractions': r'dozen|couple|pair|half|quarter|third|fourth|fifth|sixth|seventh|eighth|ninth|tenth',\n",
    "            'times': r'\\d{1,2}:\\d{2}(?::\\d{2})?(?:\\s*[AaPp][Mm])?',\n",
    "            'am_pm': r'\\d{1,2}\\s*[AaPp][Mm]',\n",
    "            'time_words': r'noon|midnight|morning|afternoon|evening|night',\n",
    "            'time_modifiers': r'(?:early|late|mid|around|about|before|after)\\s+(?:morning|afternoon|evening|night)',\n",
    "            'time_units': r'(?:milli)?seconds?|minutes?|hours?|days?|weeks?|months?|years?|decades?|centuries?',\n",
    "            'time_periods': r'fortnight|semester|quarter|annual|biannual|biennial',\n",
    "            'frequencies': r'daily|weekly|monthly|yearly|hourly',\n",
    "            'dates': r'\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}|\\d{4}[/-]\\d{1,2}[/-]\\d{1,2}|\\d{4}-\\d{2}-\\d{2}',\n",
    "            'months': r'(?:January|February|March|April|May|June|July|August|September|October|November|December|Jan|Feb|Mar|Apr|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec)',\n",
    "            'days': r'(?:Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday|Mon|Tue|Tues|Wed|Thu|Thur|Thurs|Fri|Sat|Sun)',\n",
    "            'seasons': r'(?:Spring|Summer|Fall|Autumn|Winter)',\n",
    "            'relative_time': r'(?:last|next|this)\\s+(?:week|month|year|decade|century)',\n",
    "            'relative_days': r'(?:yesterday|today|tomorrow)',\n",
    "            'time_distance': r'(?:ago|from now)',\n",
    "            'decades': r'(?:19|20)\\d0s|(?:nineteen|twenty)-(?:(?:twen|thir|for|fif|six|seven|eigh|nine)ties)',\n",
    "            'measurements': r'\\d+\\s*(?:km|m|cm|mm|mi|ft|in|kg|g|mg|lb|oz)',\n",
    "            'measurement_words': r'(?:kilo|centi|milli)?(?:meters?|grams?|litres?|liters?)',\n",
    "            'currency': r'\\$\\d+(?:\\.\\d{2})?|\\d+\\s*(?:dollars?|cents?|euros?|pounds?)',\n",
    "            'percentages': r'\\d+(?:\\.\\d+)?%|\\d+\\s+percent'\n",
    "        }\n",
    "        \n",
    "        return {name: re.compile(pattern, re.IGNORECASE) for name, pattern in pattern_strings.items()}\n",
    "    \n",
    "    @lru_cache(maxsize=10000)\n",
    "    def _process_text(self, text: str) -> Tuple[str, int]:\n",
    "        \"\"\"Process a single text string with caching\"\"\"\n",
    "        text = str(text)\n",
    "        replacements = 0\n",
    "        \n",
    "        # Convert text to hashable form for caching\n",
    "        cache_key = hash(text)\n",
    "        if cache_key in self.result_cache:\n",
    "            return self.result_cache[cache_key]\n",
    "        \n",
    "        modified_text = text\n",
    "        for pattern in self.patterns.values():\n",
    "            matches = pattern.findall(modified_text)\n",
    "            replacements += len(matches)\n",
    "            modified_text = pattern.sub('number', modified_text)\n",
    "        \n",
    "        result = (modified_text, replacements)\n",
    "        self.result_cache[cache_key] = result\n",
    "        return result\n",
    "    \n",
    "    def _process_batch(self, texts: List[str]) -> List[Tuple[str, int, int]]:\n",
    "        \"\"\"Process a batch of texts and return original word counts\"\"\"\n",
    "        return [(self._process_text(text)[0],  # modified text\n",
    "                self._process_text(text)[1],  # replacement count\n",
    "                len(str(text).split()))  # original word count\n",
    "               for text in texts]\n",
    "    \n",
    "    def analyze_texts(self, df: pd.DataFrame, text_column: str = 'text', \n",
    "                     batch_size: int = None) -> Tuple[pd.DataFrame, pd.DataFrame, Dict]:\n",
    "        \"\"\"\n",
    "        Analyze texts with batch processing and parallel execution\n",
    "        \"\"\"\n",
    "        # Determine optimal batch size if not provided\n",
    "        if batch_size is None:\n",
    "            batch_size = max(100, len(df) // (self.cpu_count * 4))\n",
    "        \n",
    "        # Initialize storage\n",
    "        modified_texts = []\n",
    "        replacement_counts = []\n",
    "        original_word_counts = []\n",
    "        modified_word_counts = []\n",
    "        \n",
    "        # Process in batches with progress bar\n",
    "        texts = df[text_column].tolist()\n",
    "        batches = [texts[i:i + batch_size] for i in range(0, len(texts), batch_size)]\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=self.cpu_count) as executor:\n",
    "            results = list(tqdm(\n",
    "                executor.map(self._process_batch, batches),\n",
    "                total=len(batches),\n",
    "                desc=\"Processing text batches\"\n",
    "            ))\n",
    "        \n",
    "        # Flatten results\n",
    "        for batch_result in results:\n",
    "            for modified_text, replacements, orig_word_count in batch_result:\n",
    "                modified_texts.append(modified_text)\n",
    "                replacement_counts.append(replacements)\n",
    "                original_word_counts.append(orig_word_count)\n",
    "                modified_word_counts.append(len(modified_text.split()))\n",
    "        \n",
    "        # Create modified DataFrame\n",
    "        df_modified = df.copy()\n",
    "        df_modified[text_column] = modified_texts\n",
    "        \n",
    "        # Create analysis DataFrame\n",
    "        analysis_df = pd.DataFrame({\n",
    "            'original_text': df[text_column],\n",
    "            'modified_text': modified_texts,\n",
    "            'replacements': replacement_counts,\n",
    "            'original_word_count': original_word_counts,\n",
    "            'modified_word_count': modified_word_counts,\n",
    "            'word_count_difference': np.subtract(modified_word_counts, original_word_counts)\n",
    "        })\n",
    "        \n",
    "        # Calculate results\n",
    "        results = {\n",
    "            'word_counts': {\n",
    "                'original': sum(original_word_counts),\n",
    "                'modified': sum(modified_word_counts)\n",
    "            },\n",
    "            'replacement_stats': {\n",
    "                'total_replacements': sum(replacement_counts),\n",
    "                'total_rows': len(df),\n",
    "                'avg_replacements_per_row': sum(replacement_counts) / len(df) if len(df) > 0 else 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Cache results to disk\n",
    "        cache_file = os.path.join(self.cache_dir, f'analysis_cache_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.joblib')\n",
    "        joblib.dump((df_modified, analysis_df, results), cache_file)\n",
    "        \n",
    "        return df_modified, analysis_df, results\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear both memory and disk caches\"\"\"\n",
    "        self.result_cache.clear()\n",
    "        self._process_text.cache_clear()\n",
    "        for file in os.listdir(self.cache_dir):\n",
    "            if file.startswith('analysis_cache_'):\n",
    "                os.remove(os.path.join(self.cache_dir, file))\n",
    "\n",
    "def print_analysis(results: Dict):\n",
    "    \"\"\"Print the analysis results in a readable format\"\"\"\n",
    "    print(\"\\nText Replacement Analysis:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Metric':<25} {'Value':<15}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    print(f\"{'Total replacements':<25} {results['replacement_stats']['total_replacements']:<15}\")\n",
    "    print(f\"{'Total rows processed':<25} {results['replacement_stats']['total_rows']:<15}\")\n",
    "    print(f\"{'Avg replacements/row':<25} {results['replacement_stats']['avg_replacements_per_row']:.2f}\")\n",
    "    \n",
    "    print(\"\\nWord Counts:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Original text total: {results['word_counts']['original']}\")\n",
    "    print(f\"Modified text total: {results['word_counts']['modified']}\")\n",
    "    print(f\"Difference: {results['word_counts']['modified'] - results['word_counts']['original']}\")\n",
    "\n",
    "\n",
    "#expanded = pd.read_csv('/Users/notagain/data_preprocessing/text_cleaning/expanded.csv')\n",
    "replacer = TextPatternReplacer()\n",
    "modified_df_2, analysis, results = replacer.analyze_texts(modified_df_1)\n",
    "print_analysis(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60875/60875 [00:05<00:00, 11909.52it/s]\n"
     ]
    }
   ],
   "source": [
    "word_counts = count_words(modified_df_2, 'text')\n",
    "word_counts.to_csv('/Users/notagain/data_preprocessing/text_cleaning/semi_word_counts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/notagain/data_preprocessing/text_cleaning\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def process_hyphens(df, text_column='text'):\n",
    "    \"\"\"\n",
    "    Process hyphens in text:\n",
    "    1. Split hyphenated words (except e-mail → email)\n",
    "    2. Remove standalone hyphens with spaces around them\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with text to analyze\n",
    "        text_column: Name of the text column\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (Modified DataFrame, Analysis DataFrame, Results dictionary)\n",
    "    \"\"\"\n",
    "    # Copy DataFrame to avoid modifying original\n",
    "    df_modified = df.copy()\n",
    "    \n",
    "    # Initialize storage for analysis\n",
    "    hyphenated_words = []\n",
    "    standalone_hyphens = []\n",
    "    \n",
    "    # Compile regex patterns\n",
    "    email_pattern = re.compile(r'\\be-mails?\\b', re.IGNORECASE)\n",
    "    hyphenated_pattern = re.compile(r'\\b\\w+[-]\\w+\\b')\n",
    "    standalone_pattern = re.compile(r'\\s+-\\s+')  # matches \" - \"\n",
    "    \n",
    "    def process_text(text):\n",
    "        \"\"\"Process individual text, handling both hyphenated words and standalone hyphens\"\"\"\n",
    "        text = str(text)\n",
    "        \n",
    "        # Count standalone hyphens before removing them\n",
    "        standalone_count = len(standalone_pattern.findall(text))\n",
    "        \n",
    "        # Handle e-mail special case\n",
    "        text = email_pattern.sub(lambda m: \n",
    "            'email' if m.group().lower() == 'e-mail' else 'emails', \n",
    "            text\n",
    "        )\n",
    "        \n",
    "        # Find hyphenated words before processing\n",
    "        found_words = hyphenated_pattern.findall(text)\n",
    "        \n",
    "        # Replace hyphenated words with space-separated words\n",
    "        for word in found_words:\n",
    "            if not email_pattern.match(word):  # Skip if it's an e-mail\n",
    "                split_version = word.replace('-', ' ')\n",
    "                text = text.replace(word, split_version)\n",
    "        \n",
    "        # Remove standalone hyphens with spaces\n",
    "        text = standalone_pattern.sub(' ', text)\n",
    "        \n",
    "        return text, found_words, standalone_count\n",
    "    \n",
    "    # Process each row\n",
    "    modified_texts = []\n",
    "    for text in df[text_column]:\n",
    "        modified_text, found_words, standalone_count = process_text(text)\n",
    "        modified_texts.append(modified_text)\n",
    "        hyphenated_words.extend(found_words)\n",
    "        standalone_hyphens.append(standalone_count)\n",
    "    \n",
    "    # Update modified DataFrame\n",
    "    df_modified[text_column] = modified_texts\n",
    "    \n",
    "    # Create analysis DataFrame\n",
    "    word_counts = Counter(hyphenated_words)\n",
    "    analysis_df = pd.DataFrame({\n",
    "        'original_word': list(word_counts.keys()),\n",
    "        'split_into': [' '.join(word.split('-')) for word in word_counts.keys()],\n",
    "        'count': list(word_counts.values())\n",
    "    }).sort_values('count', ascending=False)\n",
    "    \n",
    "    # Calculate results\n",
    "    results = {\n",
    "        'total_hyphenated_found': len(hyphenated_words),\n",
    "        'unique_hyphenated': len(word_counts),\n",
    "        'total_standalone_hyphens': sum(standalone_hyphens),\n",
    "        'email_conversions': sum(1 for text in df[text_column] \n",
    "                               if re.search(email_pattern, str(text))),\n",
    "        'most_common': list(word_counts.most_common(5))\n",
    "    }\n",
    "    \n",
    "    return df_modified, analysis_df, results\n",
    "\n",
    "def print_analysis(results, analysis_df):\n",
    "    \"\"\"Print analysis of hyphen processing\"\"\"\n",
    "    print(\"\\nHyphen Processing Analysis:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Total hyphenated words found: {results['total_hyphenated_found']}\")\n",
    "    print(f\"Unique hyphenated words: {results['unique_hyphenated']}\")\n",
    "    print(f\"Standalone hyphens removed: {results['total_standalone_hyphens']}\")\n",
    "    print(f\"E-mail conversions performed: {results['email_conversions']}\")\n",
    "    \n",
    "    if len(analysis_df) > 0:\n",
    "        print(\"\\nMost common hyphenated words and their splits:\")\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"{'Original':<20} {'Split Into':<25} {'Count':<10}\")\n",
    "        print(\"-\" * 60)\n",
    "        for _, row in analysis_df.head().iterrows():\n",
    "            print(f\"{row['original_word']:<20} {row['split_into']:<25} {row['count']:<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyphen Processing Analysis:\n",
      "------------------------------------------------------------\n",
      "Total hyphenated words found: 6723\n",
      "Unique hyphenated words: 3066\n",
      "Standalone hyphens removed: 3228\n",
      "E-mail conversions performed: 188\n",
      "\n",
      "Most common hyphenated words and their splits:\n",
      "------------------------------------------------------------\n",
      "Original             Split Into                Count     \n",
      "------------------------------------------------------------\n",
      "number-number        number number             316       \n",
      "follow-up            follow up                 132       \n",
      "number-star          number star               105       \n",
      "pick-up              pick up                   81        \n",
      "on-line              on line                   71        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60875/60875 [00:05<00:00, 11297.27it/s]\n"
     ]
    }
   ],
   "source": [
    "modified_df_no_hyp, analysis, results = process_hyphens(modified_df_2)\n",
    "\n",
    "# Print analysis\n",
    "print_analysis(results, analysis)\n",
    "\n",
    "modified_df_3 = count_words(modified_df_no_hyp, 'text')\n",
    "modified_df_3.to_csv('/Users/notagain/data_preprocessing/text_cleaning/unhyphen.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 61/61 [00:00<00:00, 36813.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Character Removal Analysis:\n",
      "------------------------------------------------------------\n",
      "Total rows processed: 60875\n",
      "Total characters removed: 109717\n",
      "Average characters removed per row: 1.80\n",
      "\n",
      "Characters removed by type:\n",
      "------------------------------------------------------------\n",
      "Hyphen                3662\n",
      "Comma                94692\n",
      "Quotes               11363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import multiprocessing\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CharacterRemover:\n",
    "    \"\"\"Efficient character removal processor with batching and analysis\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Define all character variants to remove\n",
    "        self.chars_to_remove = {\n",
    "            'hyphen': [\n",
    "                '\\u002D',  # hyphen-minus\n",
    "                '\\u2010',  # hyphen\n",
    "                '\\u2011',  # non-breaking hyphen\n",
    "                '\\u2012',  # figure dash\n",
    "                '\\u2013',  # en dash\n",
    "                '\\u2014',  # em dash\n",
    "                '\\u2015'   # horizontal bar\n",
    "            ],\n",
    "            'comma': [\n",
    "                '\\u002C',  # standard comma\n",
    "                '\\u201A'   # single low-9 quotation mark (sometimes used as comma)\n",
    "            ],\n",
    "            'quotes': [\n",
    "                '\\u0022',  # standard double quote\n",
    "                '\\u201C',  # left double quote\n",
    "                '\\u201D',  # right double quote\n",
    "                '\\u201E',  # double low-9 quote\n",
    "                '\\u201F',  # double high-reversed-9 quote\n",
    "                '\\u2033',  # double prime\n",
    "                '\\u2034',  # triple prime\n",
    "                '\\u2057'   # quadruple prime\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Create regex pattern for all characters\n",
    "        all_chars = ''.join([char for sublist in self.chars_to_remove.values() \n",
    "                           for char in sublist])\n",
    "        self.remove_pattern = re.compile(f'[{re.escape(all_chars)}]')\n",
    "        \n",
    "        # Set optimal batch size based on CPU count\n",
    "        self.cpu_count = multiprocessing.cpu_count()\n",
    "        self.default_batch_size = 1000\n",
    "    \n",
    "    def process_batch(self, texts: List[str]) -> List[Tuple[str, Dict]]:\n",
    "        \"\"\"Process a batch of texts\"\"\"\n",
    "        results = []\n",
    "        for text in texts:\n",
    "            # Count original occurrences\n",
    "            counts = {\n",
    "                char_type: sum(text.count(char) for char in chars)\n",
    "                for char_type, chars in self.chars_to_remove.items()\n",
    "            }\n",
    "            \n",
    "            # Remove characters\n",
    "            cleaned_text = self.remove_pattern.sub('', text)\n",
    "            \n",
    "            results.append((cleaned_text, counts))\n",
    "        return results\n",
    "\n",
    "    def remove_characters(self, df: pd.DataFrame, text_column: str = 'text', \n",
    "                         batch_size: int = None) -> Tuple[pd.DataFrame, pd.DataFrame, Dict]:\n",
    "        \"\"\"\n",
    "        Remove specified characters from text with batch processing.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            text_column: Name of text column\n",
    "            batch_size: Optional custom batch size\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (Modified DataFrame, Analysis DataFrame, Results dictionary)\n",
    "        \"\"\"\n",
    "        # Copy DataFrame\n",
    "        df_modified = df.copy()\n",
    "        \n",
    "        # Set batch size\n",
    "        batch_size = batch_size or self.default_batch_size\n",
    "        \n",
    "        # Prepare batches\n",
    "        texts = df[text_column].astype(str).tolist()\n",
    "        batches = [texts[i:i + batch_size] \n",
    "                  for i in range(0, len(texts), batch_size)]\n",
    "        \n",
    "        # Process batches in parallel\n",
    "        all_results = []\n",
    "        total_counts = {char_type: 0 for char_type in self.chars_to_remove.keys()}\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=self.cpu_count) as executor:\n",
    "            futures = [executor.submit(self.process_batch, batch) \n",
    "                      for batch in batches]\n",
    "            \n",
    "            # Process results with progress bar\n",
    "            for future in tqdm(futures, desc=\"Processing batches\"):\n",
    "                batch_results = future.result()\n",
    "                all_results.extend(batch_results)\n",
    "        \n",
    "        # Unzip results\n",
    "        cleaned_texts, char_counts = zip(*all_results)\n",
    "        \n",
    "        # Update DataFrame\n",
    "        df_modified[text_column] = cleaned_texts\n",
    "        \n",
    "        # Calculate total counts\n",
    "        for counts in char_counts:\n",
    "            for char_type, count in counts.items():\n",
    "                total_counts[char_type] += count\n",
    "        \n",
    "        # Create analysis DataFrame\n",
    "        analysis_df = pd.DataFrame({\n",
    "            'original_text': df[text_column],\n",
    "            'cleaned_text': cleaned_texts,\n",
    "            'chars_removed': [len(orig) - len(clean) \n",
    "                            for orig, clean in zip(df[text_column], cleaned_texts)]\n",
    "        })\n",
    "        \n",
    "        # Compile results\n",
    "        results = {\n",
    "            'total_rows_processed': len(df),\n",
    "            'total_chars_removed': sum(analysis_df['chars_removed']),\n",
    "            'character_counts': total_counts,\n",
    "            'avg_chars_removed': sum(analysis_df['chars_removed']) / len(df)\n",
    "        }\n",
    "        \n",
    "        return df_modified, analysis_df, results\n",
    "\n",
    "def print_removal_analysis(results: Dict):\n",
    "    \"\"\"Print analysis of character removal\"\"\"\n",
    "    print(\"\\nCharacter Removal Analysis:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Total rows processed: {results['total_rows_processed']}\")\n",
    "    print(f\"Total characters removed: {results['total_chars_removed']}\")\n",
    "    print(f\"Average characters removed per row: {results['avg_chars_removed']:.2f}\")\n",
    "    \n",
    "    print(\"\\nCharacters removed by type:\")\n",
    "    print(\"-\" * 60)\n",
    "    for char_type, count in results['character_counts'].items():\n",
    "        print(f\"{char_type.title():<15} {count:>10}\")\n",
    "\n",
    "remover = CharacterRemover()\n",
    "modified_df_4, analysis, results = remover.remove_characters(modified_df_no_hyp)\n",
    "\n",
    "# Print analysis\n",
    "print_removal_analysis(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60875/60875 [00:05<00:00, 11258.21it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "count_modified_df_4 = count_words(modified_df_4, 'text')\n",
    "\n",
    "count_modified_df_4.to_csv('/Users/notagain/data_preprocessing/text_cleaning/modified_df_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def clean_text(df, text_column='text'):\n",
    "    \"\"\"\n",
    "    Clean text with guaranteed spaces around punctuation.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with text to analyze\n",
    "        text_column: Name of text column to process\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (Modified DataFrame, Analysis DataFrame, Results dictionary)\n",
    "    \"\"\"\n",
    "    # Copy DataFrame to avoid modifying original\n",
    "    df_modified = df.copy()\n",
    "    \n",
    "    # Initialize counters for analysis\n",
    "    replacements = {\n",
    "        'periods': 0,\n",
    "        'quotes': 0,\n",
    "        'ampersands': 0,\n",
    "        'commas': 0,\n",
    "        'currency': 0,\n",
    "        'parentheses': 0,\n",
    "        'exclamations': 0,\n",
    "        'extra_spaces': 0,\n",
    "        'slashes': 0\n",
    "    }\n",
    "    \n",
    "    def process_text(text):\n",
    "        \"\"\"Process individual text with all cleaning rules\"\"\"\n",
    "        text = str(text)\n",
    "        original_length = len(text)\n",
    "        \n",
    "        # Store original counts\n",
    "        replacements['quotes'] += text.count('\"') + text.count(\"'\") + text.count('\"') + text.count('\"')\n",
    "        replacements['ampersands'] += text.count('&')\n",
    "        replacements['commas'] += text.count(',')\n",
    "        replacements['periods'] += text.count('.')\n",
    "        replacements['parentheses'] += text.count('(') + text.count(')') + text.count('[') + text.count(']')\n",
    "        replacements['exclamations'] += text.count('!')\n",
    "        replacements['slashes'] += text.count('/')\n",
    "        \n",
    "        # 1. Remove ALL quotation marks\n",
    "        text = re.sub(r'[\"\"\\'\"]', '', text)\n",
    "        \n",
    "        # 2. Handle number/number pattern\n",
    "        text = re.sub(r'\\b\\d+/\\d+\\b', 'number', text)\n",
    "        \n",
    "        # 3. Handle ellipsis first (preserve it)\n",
    "        text = re.sub(r'\\.{3}', 'ELLIPSIS_PLACEHOLDER', text)\n",
    "        \n",
    "        # 4. Guarantee space before and after every comma\n",
    "        text = re.sub(r'\\s*,\\s*', ' , ', text)\n",
    "        \n",
    "        # 5. Guarantee space before and after every period (except in numbers)\n",
    "        text = re.sub(r'(?<!\\d)\\s*\\.\\s*(?!\\d)', ' . ', text)\n",
    "        \n",
    "        # 6. Replace & with 'and'\n",
    "        text = re.sub(r'&', 'and', text)\n",
    "        \n",
    "        # 7. Remove currency symbols\n",
    "        text = re.sub(r'[$€£¥¢]', '', text)\n",
    "        \n",
    "        # 8. Remove parentheses\n",
    "        text = re.sub(r'[\\(\\)\\[\\]]', '', text)\n",
    "        \n",
    "        # 9. Add space before exclamation points\n",
    "        text = re.sub(r'(?<=[^\\s])!', ' !', text)\n",
    "        \n",
    "        # 10. Restore ellipsis\n",
    "        text = text.replace('ELLIPSIS_PLACEHOLDER', '...')\n",
    "        \n",
    "        # 11. Clean up extra spaces\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # Count removed extra spaces\n",
    "        replacements['extra_spaces'] += original_length - len(text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    # Process each row\n",
    "    modified_texts = []\n",
    "    for text in df[text_column]:\n",
    "        modified_text = process_text(text)\n",
    "        modified_texts.append(modified_text)\n",
    "    \n",
    "    # Update modified DataFrame\n",
    "    df_modified[text_column] = modified_texts\n",
    "    \n",
    "    # Create analysis DataFrame\n",
    "    analysis_df = pd.DataFrame({\n",
    "        'original_text': df[text_column],\n",
    "        'cleaned_text': modified_texts,\n",
    "        'chars_removed': [len(str(orig)) - len(clean) \n",
    "                         for orig, clean in zip(df[text_column], modified_texts)]\n",
    "    })\n",
    "    \n",
    "    # Calculate results\n",
    "    results = {\n",
    "        'total_rows_processed': len(df),\n",
    "        'total_chars_removed': sum(analysis_df['chars_removed']),\n",
    "        'replacements': replacements\n",
    "    }\n",
    "    \n",
    "    return df_modified, analysis_df, results\n",
    "\n",
    "def print_cleaning_analysis(results):\n",
    "    \"\"\"Print analysis of text cleaning operations\"\"\"\n",
    "    print(\"\\nText Cleaning Analysis:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Total rows processed: {results['total_rows_processed']}\")\n",
    "    print(f\"Total characters removed: {results['total_chars_removed']}\")\n",
    "    \n",
    "    print(\"\\nReplacements by type:\")\n",
    "    print(\"-\" * 60)\n",
    "    for key, value in results['replacements'].items():\n",
    "        print(f\"{key.replace('_', ' ').title():<20} {value:>10}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text batches: 100%|██████████| 61/61 [00:10<00:00,  5.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text Processing Analysis:\n",
      "------------------------------------------------------------\n",
      "Total rows processed: 60875\n",
      "Total character difference: 9876\n",
      "Average character difference per row: 0.16\n",
      "\n",
      "Replacements by type:\n",
      "------------------------------------------------------------\n",
      "Currency                   6928\n",
      "Ampersand                  2483\n",
      "Punctuation              272679\n",
      "Number Patterns          173479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60875/60875 [00:04<00:00, 12491.71it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import multiprocessing\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        # Define patterns\n",
    "        self.patterns = {\n",
    "            'currency': r'\\$\\d*\\.?\\d+|\\$',  # Matches $, $50, $50.00\n",
    "            'ampersand': r'\\s*&\\s*',  # & with optional spaces\n",
    "            'punctuation': r'\\s*([,.!?:;])\\s*',  # Common punctuation\n",
    "            'number_patterns': [\n",
    "                r'\\b\\d+/\\d+\\b',  # number/number\n",
    "                r'\\b\\w*number\\w*\\b',  # words containing number\n",
    "                r'\\b\\w*\\d+\\w*\\b',  # words with digits\n",
    "                r'\\bnumbernumber\\b',  # explicit numbernumber\n",
    "                r'\\bsho\\d+\\b'  # shonumber\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Compile regex patterns for efficiency\n",
    "        self.currency_pattern = re.compile(self.patterns['currency'])\n",
    "        self.ampersand_pattern = re.compile(self.patterns['ampersand'])\n",
    "        self.punct_pattern = re.compile(self.patterns['punctuation'])\n",
    "        self.number_patterns = [re.compile(pattern) for pattern in self.patterns['number_patterns']]\n",
    "        \n",
    "        # Set optimal batch size based on CPU count\n",
    "        self.cpu_count = multiprocessing.cpu_count()\n",
    "        self.default_batch_size = 1000\n",
    "\n",
    "    def process_text(self, text: str) -> Tuple[str, Dict]:\n",
    "        \"\"\"Process individual text with all rules\"\"\"\n",
    "        text = str(text)\n",
    "        replacements = {\n",
    "            'currency': 0,\n",
    "            'ampersand': 0,\n",
    "            'punctuation': 0,\n",
    "            'number_patterns': 0\n",
    "        }\n",
    "\n",
    "        # Step 1: Count and remove currency symbols\n",
    "        currency_matches = len(self.currency_pattern.findall(text))\n",
    "        text = self.currency_pattern.sub('', text)\n",
    "        replacements['currency'] = currency_matches\n",
    "\n",
    "        # Step 2: Replace & with 'and'\n",
    "        ampersand_matches = len(self.ampersand_pattern.findall(text))\n",
    "        text = self.ampersand_pattern.sub(' and ', text)\n",
    "        replacements['ampersand'] = ampersand_matches\n",
    "\n",
    "        # Step 3: Process all number patterns\n",
    "        for pattern in self.number_patterns:\n",
    "            matches = pattern.findall(text)\n",
    "            replacements['number_patterns'] += len(matches)\n",
    "            text = pattern.sub('number', text)\n",
    "\n",
    "        # Step 4: Add spaces around punctuation\n",
    "        punct_matches = len(self.punct_pattern.findall(text))\n",
    "        text = self.punct_pattern.sub(r' \\1 ', text)\n",
    "        replacements['punctuation'] = punct_matches\n",
    "\n",
    "        # Step 5: Clean up extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        return text, replacements\n",
    "\n",
    "    def process_batch(self, texts: List[str]) -> List[Tuple[str, Dict]]:\n",
    "        \"\"\"Process a batch of texts\"\"\"\n",
    "        return [self.process_text(text) for text in texts]\n",
    "\n",
    "    def process_dataframe(self, df: pd.DataFrame, text_column: str = 'text', \n",
    "                         batch_size: int = None) -> Tuple[pd.DataFrame, pd.DataFrame, Dict]:\n",
    "        \"\"\"\n",
    "        Process text with batch processing and analysis.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            text_column: Name of text column\n",
    "            batch_size: Optional custom batch size\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (Modified DataFrame, Analysis DataFrame, Results dictionary)\n",
    "        \"\"\"\n",
    "        df_modified = df.copy()\n",
    "        batch_size = batch_size or self.default_batch_size\n",
    "\n",
    "        # Prepare batches\n",
    "        texts = df[text_column].astype(str).tolist()\n",
    "        batches = [texts[i:i + batch_size] \n",
    "                  for i in range(0, len(texts), batch_size)]\n",
    "\n",
    "        # Process batches in parallel\n",
    "        all_results = []\n",
    "        total_replacements = {\n",
    "            'currency': 0,\n",
    "            'ampersand': 0,\n",
    "            'punctuation': 0,\n",
    "            'number_patterns': 0\n",
    "        }\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=self.cpu_count) as executor:\n",
    "            futures = [executor.submit(self.process_batch, batch) \n",
    "                      for batch in batches]\n",
    "            \n",
    "            # Process results with progress bar\n",
    "            for future in tqdm(futures, desc=\"Processing text batches\"):\n",
    "                batch_results = future.result()\n",
    "                all_results.extend(batch_results)\n",
    "\n",
    "        # Unzip results\n",
    "        processed_texts, replacements_list = zip(*all_results)\n",
    "\n",
    "        # Update DataFrame\n",
    "        df_modified[text_column] = processed_texts\n",
    "\n",
    "        # Sum up replacements\n",
    "        for rep in replacements_list:\n",
    "            for key in total_replacements:\n",
    "                total_replacements[key] += rep[key]\n",
    "\n",
    "        # Create analysis DataFrame\n",
    "        analysis_df = pd.DataFrame({\n",
    "            'original_text': df[text_column],\n",
    "            'processed_text': processed_texts,\n",
    "            'chars_difference': [len(str(orig)) - len(proc) \n",
    "                               for orig, proc in zip(df[text_column], processed_texts)]\n",
    "        })\n",
    "\n",
    "        # Compile results\n",
    "        results = {\n",
    "            'total_rows_processed': len(df),\n",
    "            'total_chars_difference': sum(analysis_df['chars_difference']),\n",
    "            'replacements': total_replacements,\n",
    "            'avg_chars_difference': sum(analysis_df['chars_difference']) / len(df)\n",
    "        }\n",
    "\n",
    "        return df_modified, analysis_df, results\n",
    "\n",
    "def print_processing_analysis(results: Dict):\n",
    "    \"\"\"Print analysis of text processing\"\"\"\n",
    "    print(\"\\nText Processing Analysis:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Total rows processed: {results['total_rows_processed']}\")\n",
    "    print(f\"Total character difference: {results['total_chars_difference']}\")\n",
    "    print(f\"Average character difference per row: {results['avg_chars_difference']:.2f}\")\n",
    "    \n",
    "    print(\"\\nReplacements by type:\")\n",
    "    print(\"-\" * 60)\n",
    "    for replacement_type, count in results['replacements'].items():\n",
    "        print(f\"{replacement_type.replace('_', ' ').title():<20} {count:>10}\")\n",
    "\n",
    "processor = TextProcessor()\n",
    "modified_df_5, analysis, results = processor.process_dataframe(modified_df_4)\n",
    "\n",
    "# Print analysis\n",
    "print_processing_analysis(results)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "count_modified_df_5 = count_words(modified_df_5, 'text')\n",
    "\n",
    "count_modified_df_5.to_csv('/Users/notagain/data_preprocessing/text_cleaning/modified_df_5.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text batches: 100%|██████████| 61/61 [00:05<00:00, 10.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text Cleaning Analysis:\n",
      "------------------------------------------------------------\n",
      "Total rows processed: 60875\n",
      "Total characters removed: 528648\n",
      "Average characters removed per row: 8.68\n",
      "\n",
      "Replacements by type:\n",
      "------------------------------------------------------------\n",
      "Number Variations               6033\n",
      "Parentheses                     7150\n",
      "Standalone Chars              122000\n",
      "Im Replacements                  271\n",
      "I Capitalizations              17412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60875/60875 [00:05<00:00, 10913.42it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class TextCleaner:\n",
    "    def __init__(self):\n",
    "        # Define all patterns as strings\n",
    "        self.number_variations = [\n",
    "            (r'\\bnumber/number\\b', 'number'),\n",
    "            (r'\\bnumber%\\b', 'number'),\n",
    "            (r'\\bnumber\\)', 'number'),\n",
    "            (r'\\bnumber\\+', 'number'),\n",
    "            (r'\\(\\s*number\\s*\\)', 'number'),\n",
    "            (r'\\bnumber\\'s?\\b', 'number'),\n",
    "            (r'%\\s*number', 'number'),\n",
    "            (r'\\(\\s*number', 'number'),\n",
    "            (r'\\bnumber\\s*[\\'\\'\\u2019]s?\\b', 'number')\n",
    "        ]\n",
    "        \n",
    "        self.parentheses_patterns = [\n",
    "            (r'\\([^)]*\\)', ''),  # remove content within parentheses\n",
    "            (r'[\\(\\)]', '')      # remove remaining parentheses\n",
    "        ]\n",
    "        \n",
    "        self.standalone_chars = r'\\s+([etisnlochgwbm\\+/])\\s+'\n",
    "        self.im_replacement = r'\\b(?:i\\'m|im)\\b'\n",
    "        self.standalone_i = r'\\s+i\\s+'\n",
    "        \n",
    "        # Compile patterns for efficiency\n",
    "        self.number_patterns = [(re.compile(pattern), repl) \n",
    "                              for pattern, repl in self.number_variations]\n",
    "        self.parentheses_patterns = [(re.compile(pattern), repl) \n",
    "                                   for pattern, repl in self.parentheses_patterns]\n",
    "        self.standalone_pattern = re.compile(self.standalone_chars, re.IGNORECASE)\n",
    "        self.im_pattern = re.compile(self.im_replacement, re.IGNORECASE)\n",
    "        self.i_pattern = re.compile(self.standalone_i)\n",
    "        \n",
    "        # Set optimal batch size\n",
    "        self.cpu_count = multiprocessing.cpu_count()\n",
    "        self.default_batch_size = 1000\n",
    "\n",
    "    def process_text(self, text: str) -> Tuple[str, Dict]:\n",
    "        \"\"\"Process individual text with all rules\"\"\"\n",
    "        text = str(text)\n",
    "        replacements = {\n",
    "            'number_variations': 0,\n",
    "            'parentheses': 0,\n",
    "            'standalone_chars': 0,\n",
    "            'im_replacements': 0,\n",
    "            'i_capitalizations': 0\n",
    "        }\n",
    "\n",
    "        # Store original length for comparison\n",
    "        original_length = len(text)\n",
    "\n",
    "        # 1. Handle all number variations\n",
    "        for pattern, repl in self.number_patterns:\n",
    "            text, count = re.subn(pattern, repl, text)\n",
    "            replacements['number_variations'] += count\n",
    "\n",
    "        # 2. Remove parentheses and their content\n",
    "        for pattern, repl in self.parentheses_patterns:\n",
    "            text, count = re.subn(pattern, repl, text)\n",
    "            replacements['parentheses'] += count\n",
    "\n",
    "        # 3. Replace \"im\" with \"i am\"\n",
    "        text, im_count = re.subn(self.im_pattern, 'i am', text)\n",
    "        replacements['im_replacements'] = im_count\n",
    "\n",
    "        # 4. Capitalize standalone \"i\"\n",
    "        text, i_count = re.subn(self.i_pattern, ' I ', text)\n",
    "        replacements['i_capitalizations'] = i_count\n",
    "\n",
    "        # 5. Remove standalone characters\n",
    "        text, char_count = re.subn(self.standalone_pattern, ' ', text)\n",
    "        replacements['standalone_chars'] = char_count\n",
    "\n",
    "        # 6. Clean up extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        return text, replacements\n",
    "\n",
    "    def process_batch(self, texts: List[str]) -> List[Tuple[str, Dict]]:\n",
    "        \"\"\"Process a batch of texts\"\"\"\n",
    "        return [self.process_text(text) for text in texts]\n",
    "\n",
    "    def clean_texts(self, df: pd.DataFrame, text_column: str = 'text', \n",
    "                   batch_size: int = None) -> Tuple[pd.DataFrame, pd.DataFrame, Dict]:\n",
    "        \"\"\"\n",
    "        Clean texts with batch processing and analysis.\n",
    "        \"\"\"\n",
    "        df_modified = df.copy()\n",
    "        batch_size = batch_size or self.default_batch_size\n",
    "\n",
    "        # Prepare batches\n",
    "        texts = df[text_column].astype(str).tolist()\n",
    "        batches = [texts[i:i + batch_size] \n",
    "                  for i in range(0, len(texts), batch_size)]\n",
    "\n",
    "        # Process batches in parallel\n",
    "        all_results = []\n",
    "        total_replacements = {\n",
    "            'number_variations': 0,\n",
    "            'parentheses': 0,\n",
    "            'standalone_chars': 0,\n",
    "            'im_replacements': 0,\n",
    "            'i_capitalizations': 0\n",
    "        }\n",
    "\n",
    "        # Process with progress bar\n",
    "        with ThreadPoolExecutor(max_workers=self.cpu_count) as executor:\n",
    "            futures = [executor.submit(self.process_batch, batch) \n",
    "                      for batch in batches]\n",
    "            \n",
    "            for future in tqdm(futures, desc=\"Processing text batches\"):\n",
    "                batch_results = future.result()\n",
    "                all_results.extend(batch_results)\n",
    "\n",
    "        # Unzip results\n",
    "        processed_texts, replacements_list = zip(*all_results)\n",
    "\n",
    "        # Update DataFrame\n",
    "        df_modified[text_column] = processed_texts\n",
    "\n",
    "        # Sum up replacements\n",
    "        for rep in replacements_list:\n",
    "            for key in total_replacements:\n",
    "                total_replacements[key] += rep[key]\n",
    "\n",
    "        # Create analysis DataFrame\n",
    "        analysis_df = pd.DataFrame({\n",
    "            'original_text': df[text_column],\n",
    "            'processed_text': processed_texts,\n",
    "            'chars_difference': [len(str(orig)) - len(proc) \n",
    "                               for orig, proc in zip(df[text_column], processed_texts)]\n",
    "        })\n",
    "\n",
    "        # Compile results\n",
    "        results = {\n",
    "            'total_rows_processed': len(df),\n",
    "            'total_chars_removed': sum(analysis_df['chars_difference']),\n",
    "            'replacements': total_replacements,\n",
    "            'avg_chars_removed': sum(analysis_df['chars_difference']) / len(df)\n",
    "        }\n",
    "\n",
    "        return df_modified, analysis_df, results\n",
    "\n",
    "def print_cleaning_analysis(results: Dict):\n",
    "    \"\"\"Print analysis of text cleaning\"\"\"\n",
    "    print(\"\\nText Cleaning Analysis:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Total rows processed: {results['total_rows_processed']}\")\n",
    "    print(f\"Total characters removed: {results['total_chars_removed']}\")\n",
    "    print(f\"Average characters removed per row: {results['avg_chars_removed']:.2f}\")\n",
    "    \n",
    "    print(\"\\nReplacements by type:\")\n",
    "    print(\"-\" * 60)\n",
    "    for key, value in results['replacements'].items():\n",
    "        print(f\"{key.replace('_', ' ').title():<25} {value:>10}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cleaner = TextCleaner()\n",
    "modified_df_6, analysis, results = cleaner.clean_texts(modified_df_5)\n",
    "\n",
    "# Print analysis\n",
    "print_cleaning_analysis(results)\n",
    "\n",
    "\n",
    "\n",
    "count_modified_df_6 = count_words(modified_df_6, 'text')\n",
    "\n",
    "count_modified_df_6.to_csv('/Users/notagain/data_preprocessing/text_cleaning/modified_df_6.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60875/60875 [00:03<00:00, 15245.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number Word Analysis:\n",
      "------------------------------------------------------------\n",
      "Total rows processed: 60875\n",
      "Total replacements made: 168276\n",
      "Unique word variations found: 375\n",
      "Average replacements per row: 2.76\n",
      "\n",
      "Top 20 original word variations:\n",
      "------------------------------------------------------------\n",
      "     original_word  count\n",
      "            number 166951\n",
      "     number/number    653\n",
      "      numbernumber    138\n",
      "          number°F     22\n",
      "            NUMBER     19\n",
      "          number°C     13\n",
      "            Number     10\n",
      "         number/lb      9\n",
      "           Numbers      7\n",
      "      number/email      5\n",
      "     number/family      5\n",
      "         number/mo      5\n",
      "       Edit#number      4\n",
      "         numberThe      4\n",
      "    number/freezer      4\n",
      "         numberoff      4\n",
      "          number/m      3\n",
      "numbernumbernumber      3\n",
      "         number/hr      3\n",
      "         numberand      3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60875/60875 [00:04<00:00, 12594.43it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def clean_number_words(df: pd.DataFrame, text_column: str = 'text') -> tuple:\n",
    "    \"\"\"\n",
    "    Replace any word containing 'number' with just 'number'.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        text_column: Name of text column\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (Modified DataFrame, Analysis DataFrame, Results dictionary)\n",
    "    \"\"\"\n",
    "    # Copy DataFrame\n",
    "    df_modified = df.copy()\n",
    "    \n",
    "    # Create pattern to match any word containing 'number'\n",
    "    # \\S* matches any non-whitespace characters\n",
    "    # \\b represents word boundary\n",
    "    pattern = r'\\b\\S*number\\S*\\b|\\b\\S*NUMBER\\S*\\b'\n",
    "    \n",
    "    # Initialize storage for analysis\n",
    "    original_words = []\n",
    "    replacements_count = []\n",
    "    \n",
    "    # Process each text\n",
    "    modified_texts = []\n",
    "    print(\"Processing texts...\")\n",
    "    \n",
    "    for text in tqdm(df[text_column].astype(str)):\n",
    "        # Find all matches in this text\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "        \n",
    "        # Store original words found\n",
    "        original_words.extend(matches)\n",
    "        \n",
    "        # Replace all variations with 'number'\n",
    "        modified_text = re.sub(pattern, 'number', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        modified_texts.append(modified_text)\n",
    "        replacements_count.append(len(matches))\n",
    "    \n",
    "    # Update DataFrame\n",
    "    df_modified[text_column] = modified_texts\n",
    "    \n",
    "    # Create word frequency analysis\n",
    "    word_freq = pd.Series(original_words).value_counts().reset_index()\n",
    "    word_freq.columns = ['original_word', 'count']\n",
    "    \n",
    "    # Create analysis DataFrame\n",
    "    analysis_df = pd.DataFrame({\n",
    "        'original_text': df[text_column],\n",
    "        'processed_text': modified_texts,\n",
    "        'replacements': replacements_count\n",
    "    })\n",
    "    \n",
    "    # Compile results\n",
    "    results = {\n",
    "        'total_rows_processed': len(df),\n",
    "        'total_replacements': sum(replacements_count),\n",
    "        'unique_variations': len(word_freq),\n",
    "        'avg_replacements_per_row': sum(replacements_count) / len(df)\n",
    "    }\n",
    "    \n",
    "    return df_modified, analysis_df, word_freq, results\n",
    "\n",
    "def print_number_analysis(results: dict, word_freq: pd.DataFrame):\n",
    "    \"\"\"Print analysis of number word cleaning\"\"\"\n",
    "    print(\"\\nNumber Word Analysis:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Total rows processed: {results['total_rows_processed']}\")\n",
    "    print(f\"Total replacements made: {results['total_replacements']}\")\n",
    "    print(f\"Unique word variations found: {results['unique_variations']}\")\n",
    "    print(f\"Average replacements per row: {results['avg_replacements_per_row']:.2f}\")\n",
    "    \n",
    "    print(\"\\nTop 20 original word variations:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(word_freq.head(20).to_string(index=False))\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "# Process the DataFrame\n",
    "modified_df_7, analysis_df, word_freq, results = clean_number_words(modified_df_6)\n",
    "\n",
    "# Print analysis\n",
    "print_number_analysis(results, word_freq)\n",
    "\n",
    "count_modified_df_7 = count_words(modified_df_7, 'text')\n",
    "\n",
    "count_modified_df_7.to_csv('/Users/notagain/data_preprocessing/text_cleaning/modified_df_7.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60875/60875 [00:08<00:00, 6822.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number Word Analysis:\n",
      "------------------------------------------------------------\n",
      "Total rows processed: 60875\n",
      "Total replacements made: 168276\n",
      "Unique word variations found: 74\n",
      "Average replacements per row: 2.76\n",
      "\n",
      "Top 20 original word variations:\n",
      "------------------------------------------------------------\n",
      "original_word  count\n",
      "       number 165703\n",
      "      number%   1583\n",
      "      #number    236\n",
      "      £number    188\n",
      "      number'     97\n",
      "      number’     65\n",
      "      number…     60\n",
      "      number*     33\n",
      "      ~number     31\n",
      "      number°     31\n",
      "      number/     28\n",
      "      +number     21\n",
      "      number€     17\n",
      "      number#     16\n",
      "      'number     16\n",
      "      €number     14\n",
      "      @number     12\n",
      "     'number'     10\n",
      "     number''      7\n",
      "      /number      7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60875/60875 [00:05<00:00, 11877.74it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def clean_number_words(df: pd.DataFrame, text_column: str = 'text') -> tuple:\n",
    "    \"\"\"\n",
    "    Replace any word containing 'number' anywhere in it with just 'number'.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        text_column: Name of text column\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (Modified DataFrame, Analysis DataFrame, Results dictionary)\n",
    "    \"\"\"\n",
    "    # Copy DataFrame\n",
    "    df_modified = df.copy()\n",
    "    \n",
    "    # Pattern to match any word containing 'number' anywhere within it\n",
    "    # \\S* matches any non-whitespace characters before and after\n",
    "    pattern = r'\\S*number\\S*|\\S*NUMBER\\S*'\n",
    "    \n",
    "    # Initialize storage for analysis\n",
    "    original_words = []\n",
    "    replacements_count = []\n",
    "    \n",
    "    # Process each text\n",
    "    modified_texts = []\n",
    "    print(\"Processing texts...\")\n",
    "    \n",
    "    for text in tqdm(df[text_column].astype(str)):\n",
    "        # Find all matches in this text\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "        \n",
    "        # Store original words found\n",
    "        original_words.extend(matches)\n",
    "        \n",
    "        # Replace all variations with 'number'\n",
    "        modified_text = re.sub(pattern, 'number', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        modified_texts.append(modified_text)\n",
    "        replacements_count.append(len(matches))\n",
    "    \n",
    "    # Update DataFrame\n",
    "    df_modified[text_column] = modified_texts\n",
    "    \n",
    "    # Create word frequency analysis\n",
    "    word_freq = pd.Series(original_words).value_counts().reset_index()\n",
    "    word_freq.columns = ['original_word', 'count']\n",
    "    \n",
    "    # Create analysis DataFrame\n",
    "    analysis_df = pd.DataFrame({\n",
    "        'original_text': df[text_column],\n",
    "        'processed_text': modified_texts,\n",
    "        'replacements': replacements_count\n",
    "    })\n",
    "    \n",
    "    # Compile results\n",
    "    results = {\n",
    "        'total_rows_processed': len(df),\n",
    "        'total_replacements': sum(replacements_count),\n",
    "        'unique_variations': len(word_freq),\n",
    "        'avg_replacements_per_row': sum(replacements_count) / len(df)\n",
    "    }\n",
    "    \n",
    "    return df_modified, analysis_df, word_freq, results\n",
    "\n",
    "def print_number_analysis(results: dict, word_freq: pd.DataFrame):\n",
    "    \"\"\"Print analysis of number word cleaning\"\"\"\n",
    "    print(\"\\nNumber Word Analysis:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Total rows processed: {results['total_rows_processed']}\")\n",
    "    print(f\"Total replacements made: {results['total_replacements']}\")\n",
    "    print(f\"Unique word variations found: {results['unique_variations']}\")\n",
    "    print(f\"Average replacements per row: {results['avg_replacements_per_row']:.2f}\")\n",
    "    \n",
    "    print(\"\\nTop 20 original word variations:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(word_freq.head(20).to_string(index=False))\n",
    "\n",
    "# Test DataFrame with various number patterns\n",
    "\n",
    "\n",
    "# Process the DataFrame\n",
    "modified_df_8, analysis_df, word_freq, results = clean_number_words(modified_df_7)\n",
    "\n",
    "# Print analysis\n",
    "print_number_analysis(results, word_freq)\n",
    "\n",
    "\n",
    "\n",
    "# Print analysis\n",
    "\n",
    "\n",
    "count_modified_df_8 = count_words(modified_df_8, 'text')\n",
    "\n",
    "count_modified_df_8.to_csv('/Users/notagain/data_preprocessing/text_cleaning/modified_df_8.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting word frequencies...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60875/60875 [00:01<00:00, 55757.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Replacing rare words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60875/60875 [00:01<00:00, 58448.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Frequency Analysis:\n",
      "------------------------------------------------------------\n",
      "Total words processed: 3764870\n",
      "Unique words found: 60303\n",
      "Words above threshold: 11019\n",
      "Words replaced with 'unknown': 49284\n",
      "Total replacements made: 105083\n",
      "Average replacements per row: 1.73\n",
      "\n",
      "Top 20 most frequent words:\n",
      "------------------------------------------------------------\n",
      "  word  frequency\n",
      "     .     236034\n",
      "number     168276\n",
      "   the     144770\n",
      "    to     111223\n",
      "   and     106624\n",
      "     a      76301\n",
      "   was      62578\n",
      "    of      47314\n",
      "   not      47247\n",
      "    it      46508\n",
      "    is      43704\n",
      "   for      42989\n",
      "    my      41522\n",
      "  that      36621\n",
      "    in      33932\n",
      "  with      32126\n",
      "  have      32025\n",
      "    on      26853\n",
      "     !      25091\n",
      "  they      24840\n",
      "\n",
      "Least frequent words (sample of 20):\n",
      "------------------------------------------------------------\n",
      "               word  frequency\n",
      "              PLUGS          1\n",
      "                EAR          1\n",
      "            Iyengar          1\n",
      "           trainer/          1\n",
      "             Biased          1\n",
      "            ovation          1\n",
      "         stickshift          1\n",
      "uncomfortable/stiff          1\n",
      "               UPPS          1\n",
      "         drumsticks          1\n",
      "            meatier          1\n",
      "              say’s          1\n",
      "            orgasms          1\n",
      "           detested          1\n",
      "          inflating          1\n",
      "       Usedlighting          1\n",
      "           jiggling          1\n",
      "            captcha          1\n",
      "            format*          1\n",
      "             Firmly          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60875/60875 [00:04<00:00, 12710.96it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "def replace_rare_words(df: pd.DataFrame, text_column: str = 'text', \n",
    "                      min_frequency: int = 10) -> Tuple[pd.DataFrame, pd.DataFrame, Dict]:\n",
    "    \"\"\"\n",
    "    Count word occurrences and replace rare words with 'unknown'.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        text_column: Name of text column\n",
    "        min_frequency: Minimum frequency threshold (default 10)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (Modified DataFrame, Word Frequency DataFrame, Results dictionary)\n",
    "    \"\"\"\n",
    "    # Copy DataFrame\n",
    "    df_modified = df.copy()\n",
    "    \n",
    "    # Initialize word counter\n",
    "    word_counter = Counter()\n",
    "    \n",
    "    print(\"Counting word frequencies...\")\n",
    "    # First pass: count all words\n",
    "    for text in tqdm(df[text_column].astype(str)):\n",
    "        # Split into words and count\n",
    "        words = text.split()\n",
    "        word_counter.update(words)\n",
    "    \n",
    "    # Create sets for quick lookup\n",
    "    common_words = {word for word, count in word_counter.items() \n",
    "                   if count >= min_frequency}\n",
    "    \n",
    "    # Create frequency DataFrame\n",
    "    freq_df = pd.DataFrame({\n",
    "        'word': list(word_counter.keys()),\n",
    "        'frequency': list(word_counter.values())\n",
    "    }).sort_values('frequency', ascending=False)\n",
    "    \n",
    "    print(\"\\nReplacing rare words...\")\n",
    "    # Second pass: replace rare words\n",
    "    modified_texts = []\n",
    "    replacement_counts = []\n",
    "    \n",
    "    for text in tqdm(df[text_column].astype(str)):\n",
    "        words = text.split()\n",
    "        replacements = 0\n",
    "        \n",
    "        # Replace rare words\n",
    "        modified_words = [\n",
    "            word if word in common_words else 'unknown'\n",
    "            for word in words\n",
    "        ]\n",
    "        \n",
    "        # Count replacements\n",
    "        replacements = sum(1 for orig, mod in zip(words, modified_words) \n",
    "                         if orig != mod)\n",
    "        \n",
    "        modified_texts.append(' '.join(modified_words))\n",
    "        replacement_counts.append(replacements)\n",
    "    \n",
    "    # Update DataFrame\n",
    "    df_modified[text_column] = modified_texts\n",
    "    \n",
    "    # Compile results\n",
    "    results = {\n",
    "        'total_words_processed': sum(word_counter.values()),\n",
    "        'unique_words': len(word_counter),\n",
    "        'words_above_threshold': len(common_words),\n",
    "        'words_replaced': len(word_counter) - len(common_words),\n",
    "        'total_replacements': sum(replacement_counts),\n",
    "        'avg_replacements_per_row': sum(replacement_counts) / len(df)\n",
    "    }\n",
    "    \n",
    "    return df_modified, freq_df, results\n",
    "\n",
    "def print_word_analysis(results: Dict, freq_df: pd.DataFrame):\n",
    "    \"\"\"Print analysis of word replacement\"\"\"\n",
    "    print(\"\\nWord Frequency Analysis:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Total words processed: {results['total_words_processed']}\")\n",
    "    print(f\"Unique words found: {results['unique_words']}\")\n",
    "    print(f\"Words above threshold: {results['words_above_threshold']}\")\n",
    "    print(f\"Words replaced with 'unknown': {results['words_replaced']}\")\n",
    "    print(f\"Total replacements made: {results['total_replacements']}\")\n",
    "    print(f\"Average replacements per row: {results['avg_replacements_per_row']:.2f}\")\n",
    "    \n",
    "    print(\"\\nTop 20 most frequent words:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(freq_df.head(20).to_string(index=False))\n",
    "    \n",
    "    print(\"\\nLeast frequent words (sample of 20):\")\n",
    "    print(\"-\" * 60)\n",
    "    print(freq_df.tail(20).to_string(index=False))\n",
    "\n",
    "modified_df_9, freq_df, results = replace_rare_words(modified_df_8, min_frequency=10)\n",
    "\n",
    "# Print analysis\n",
    "print_word_analysis(results, freq_df)\n",
    "\n",
    "count_modified_df_9 = count_words(modified_df_9, 'text')\n",
    "\n",
    "count_modified_df_9.to_csv('/Users/notagain/data_preprocessing/text_cleaning/modified_df_9.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
